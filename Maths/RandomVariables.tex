\documentclass[./EngineeringMaths.tex]{subfiles}

\begin{document}
\section[Random Variables]{Random Variables}

If a real variable $X$ is associated with an outcome of a random experiment, it is called a \textit{random variable} or a \textit{stochastic variable} or simply a \textit{variate}.


Types of Random Variables:

\begin{itemize}
\item Discrete Random Variables
\item Continuous Random Variables
\end{itemize}

% = = = = = = PDF = = = = = =
\subsection[Probability Distribution Function]{Probability Distribution Function (pdf)}
This is a function that denotes the probability of a given event as a continuous/discrete function of $f(x)$ where $x \in \mathbb{R}$.

% = = = = = = CDF = = = = = =
\subsection[Cumulative Distribution Function]{Cumulative Distribution Function (cdf)}
This is a function that denotes the sum of probability of a given event as a continuous/discrete function of $F(X)$ where $X$ will be $ \leq x$ .

% = = = = = = STATISTICAL TERMS = = = = = = 
\subsection{Statistical Terminologies}

\begin{itemize}
\item Mean (Expectation of $x$): Denoted by \fbox{$E(x)$}
\item Variance: Denoted by \fbox{$V(x)$ or $\sigma^2$}
\item Standard Deviation: Denoted by \fbox{$\sigma$}
\end{itemize}
\vspace{1cm}
\begin{center}
{\renewcommand{\arraystretch}{2}
\begin{tabular}{c|c|c|}
\cline{2-3}
& Discrete Random Variables & Continuous Random Variables \\
\cline{2-3}
\multicolumn{1}{c|}{$\mu$ or $E(x)$} & $\sum\limits_{i=1}^n x_i P(x_i)$ &  $\int\limits_{-\infty}^\infty x P(x) \, dx $ \\
\cline{2-3}
\multicolumn{1}{c|}{$E(x^2)$} & $\sum\limits_{i=1}^n x_i^2 P(x_i)$ &  $\int\limits_{-\infty}^\infty x^2 P(x) \, dx $ \\
\cline{2-3}
\multicolumn{1}{c}{$E(x-\mu)^2$ or $\sigma^2$} & \multicolumn{2}{|c|}{$V(x) = E(x^2) - E(x)^2$} \\
\cline{2-3}
\end{tabular}}
\end{center}
\newpage

% = = = = = = CHEBYSHEV'S INEQUALITY = = = = = =
\subsection{Chebyshev's Inequality}
Let $x$ be a random variable with $E(x) = \mu$ and $c$ be any real number, then if $E(x-c)^2$ is finite and is any positive number,

\begin{center}
\fbox{$P\{|x-c|\geq \varepsilon\} \leq \frac{E(x-c)^2}{\varepsilon^2}$} \\ \vspace{0.3cm}
OR \\ \vspace{0.3cm}
\fbox{$P\{|x-c|\leq \varepsilon\} \geq 1 - \frac{E(x-c)^2}{\varepsilon^2}$}
\end{center}

If \fbox{$c=\mu$} then,

\begin{center}
    $P\{|x-c|\geq \varepsilon\} \leq \frac{V(x)}{\varepsilon^2}$
\end{center}

If \fbox{$c=\mu$} \& \fbox{$\varepsilon=k\sigma$}  then,
\begin{center}
    $P\{|x-c|\geq \varepsilon\} \leq \frac{1}{k^2}$
\end{center}

% = = = = = = MARKOV's INEQUALITY = = = = = =
\subsection{Markov's Inequality}
For $a>0$, 
\begin{center}
\fbox{$P\{x\geq a\} \leq \frac{E(x)}{a}$}
\end{center}

% = = = = = = UNIFORM DISTRIBUTION = = = = = =
\subsection{Uniform Distribution}
If $X$ is a continuous random variable defined over an interval $[a,b]$ and having probability distribution function

$$
f(x) = \left\{
    \begin{array}{cr}
        \frac{1}{b-a} & a\leq x \leq b \\
        0 & \mbox{elsewhere}
    \end{array}
\right.
$$
then we say $X$ has uniform distribution. Denoted as follows: $X \sim \mathbf{U}(a,b)  $

We define the mean,variance as follows:
\begin{itemize}
    \item $E(x) = \frac{a+b}{2}$
    \item $E(x^2) = \frac{1}{3}(a^2+b^2+ab)$
    \item $V(x) = \frac{(b-a)^2}{12}$
\end{itemize}

% = = = = = = 2 DIMENSIONAL RV = = = = = = 
\subsection{Two Dimensional Random Variables}
Let $x,y$ be 2 random variables distributed in a 2 dimensional space $S$.
$$x,y \rightarrow \mbox{random variable}$$
$$x(S) = x_1,x_2\dots x_n \hspace{1cm} y(S) = y_1,y_2\dots y_m$$
then we define P(x=$x_i$,y=$y_j$) = $P_{ij}$ such that,
\begin{itemize}
    \item $P_{ij} \geq 0$
    \item $\sum\limits_{i=1}^n \sum\limits_{j=1}^m P_{ij} = 1 $
\end{itemize}

% - - - - - - JOINT PROBABILITY FUNCTION - - - - - - 
\subsubsection{Joint Probability Function}
also known as Joint Probability Mass Function is function on the set $(x_i,y_j,P_{ij})$.

\begin{center}
{\renewcommand{\arraystretch}{1.1}
\begin{tabular}{c|cccccc|c}
\cline{1-7}
\multicolumn{1}{|c|}{\diagbox{$x_i$}{$y_j$}} & $y_1$ & $y_2$ & \multicolumn{3}{c}{$\dots$} & $y_m$ & \\ \hline
\multicolumn{1}{|c|}{$x_1$} & $P_{11}$ & $P_{12}$ & \multicolumn{3}{c}{$\dots$} & $P_{1m}$ & \multicolumn{1}{c|}{$f(x_1)$} \\ \hline
\multicolumn{1}{|c|}{$x_2$} & $P_{21}$ & $P_{22}$ & \multicolumn{3}{c}{$\dots$} & $P_{2m}$ & \multicolumn{1}{c|}{$f(x_2)$} \\ \hline
\multicolumn{1}{|c|}{$\vdots$} & $\vdots$ & $\vdots$ & \multicolumn{3}{c}{$\dots$} & $\vdots$ & \multicolumn{1}{c|}{$\vdots$} \\
\multicolumn{1}{|c|}{$\vdots$}  & $\vdots$ & $\vdots$ & \multicolumn{3}{c}{$\dots$} & $\vdots$ & \multicolumn{1}{c|}{$\vdots$} \\ \hline
\multicolumn{1}{|c|}{$x_n$} & $P_{n1}$ & $P_{n2}$ & \multicolumn{3}{c}{$\dots$} & $P_{nm}$ & \multicolumn{1}{c|}{$f(x_n)$} \\ \hline
& $g(y_1)$ & $g(y_2)$ & \multicolumn{3}{c}{$\dots$} & $g(y_m)$ & \multicolumn{1}{c|}{$1$} \\ \cline{2-8}

\end{tabular}}
\end{center}

We define a few terms such as $f(x_i)$ and $g(y_j)$ for the probability function of two variables $f(x,y)$.

\begin{center}
\fbox{$f(x_i) = \sum\limits_{j=1}^m P_{ij}$ \quad ; \quad $f(y_j) = \sum\limits_{i=1}^n P_{ij}$}
\end{center}

Based on the terms mentioned above, we have following formulae,

\begin{center}
{\renewcommand{\arraystretch}{2}
\begin{tabular}{c|c|c|}
\cline{2-3}
& Discrete Random Variables & Continuous Random Variables \\
\cline{2-3}
\multicolumn{1}{c|}{$E(x)$} & $\sum\limits_{i=1}^n x_i Pfx_i)$ &  $\int\limits_{-\infty}^\infty\int\limits_{-\infty}^\infty x f(x,y) \, dx\,dy $ \\
\cline{2-3}
\multicolumn{1}{c|}{$E(y)$} & $\sum\limits_{j=1}^m y_j g(y_j)$ &  $\int\limits_{-\infty}^\infty\int\limits_{-\infty}^\infty y f(x,y) \, dx\,dy $ \\
\cline{2-3}
\multicolumn{1}{c|}{$E(xy)$} & $\sum\limits_{1\leq i\leq n,1\leq j\leq m}^n x_i\,y_i P_{ij}$ &  $\int\limits_{-\infty}^\infty\int\limits_{-\infty}^\infty xy f(x,y) \, dx\,dy $ \\
\cline{2-3}
\multicolumn{1}{c|}{$E(x^2)$} & $\sum\limits_{i=1}^n x_i^2 Pfx_i)$ &  $\int\limits_{-\infty}^\infty\int\limits_{-\infty}^\infty x^2 f(x,y) \, dx\,dy $ \\
\cline{2-3}
\multicolumn{1}{c|}{$E(y^2)$} & $\sum\limits_{j=1}^m y_j^2 g(y_j)$ &  $\int\limits_{-\infty}^\infty\int\limits_{-\infty}^\infty y^2 f(x,y) \, dx\,dy $ \\
\cline{2-3}
\end{tabular}}
\end{center}

For a crv, $(x,y)$ is associated with function $f(X,y)$ such that,
\begin{itemize}
\item $f(x,y) \geq 0$
\item $\int\limits_{-\infty}^\infty\int\limits_{-\infty}^\infty f(x,y) \,dx\,dy = 1$
\end{itemize}
$f(x,y)$ is known as the joint probability density function.

% - - - - - - COVARIANCE - - - - - -
\subsubsection{Covariance and Correlation Coefficient}
The relation of the two variables $x$ and $y$ can be defined by covariance which when $+ve$ means that they are directly proportional and when $-ve$ means inversely proportional. When the covariance is 0, it means that the 2 variable are completely unrelated.

$$Cov(x,y) = E(xy) - E(x)\,E(y)$$

This is called Measure of Correlation.

% - - - - - - CORELATION COEFFICIENT - - - - - -
\subsubsection*{Correlation Coefficient}
The numerical measure of correlation is called the coefficient of correlation and is defined by the relation: 

$$ r(x,y) = r_{xy} = \frac{Cov(x,y)}{\sigma_x\sigma_y} = \frac{E(xy) - E(x)\,E(y)}{\sqrt{V(x)\,V(y)}} $$

\end{document}